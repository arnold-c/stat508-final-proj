---
title: "STAT 508 Final Project Analysis"
author: "Callum Arnold"
output:
    html_notebook:
        code_folding: hide
        toc: yes
        toc_float: yes
---

# Set Up

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, fig.width = 14)
```

```{r}
library(tidyverse)
library(tidymodels)
library(themis)
library(kknn)
library(here)
library(kableExtra)
library(hrbrthemes)
library(janitor)

RNGkind(sample.kind = "Rounding")
set.seed(1)

theme_set(theme_ipsum())
```

```{r}
credit <- as_tibble(read_csv(here("data", "creditcard.csv")))
```

```{r}
credit <- credit %>%
  mutate(
    log_amount = log(Amount + 1),
    Class = case_when(
      Class == 0 ~ "None",
      Class == 1 ~ "Fraud"
    )
  ) %>%
  mutate(across(.cols = Class, .fns = factor)) %>%
  select(-Amount)
```


```{r}
tabyl(credit$Class)
```

# Pre-Processing

Because there's a severe class imbalance, we should use subsampling (either oversampling or undersampling).

Subsampling has a few important points regarding the [workflow](https://www.tidymodels.org/learn/models/sub-sampling/):

- It is extremely important that subsampling occurs inside of resampling. Otherwise, the resampling process can produce poor estimates of model performance.
- The subsampling process should only be applied to the analysis set. The assessment set should reflect the event rates seen “in the wild” and, for this reason, the skip argument to step_downsample() and other subsampling recipes steps has a default of TRUE.

```{r}
set.seed(1234)
credit_split <- initial_split(credit, strata = "Class", prop = 0.8)

credit_train <- training(credit_split)
credit_test <- testing(credit_split)

tabyl(credit_train$Class)
tabyl(credit_test$Class)
```

```{r}
# Create fold so same folds analyzed for every method
set.seed(1234)
credit_folds <- vfold_cv(credit_train, v = 10)
```

```{r}
# Create recipe so all folds are undergo same pre-processing
credit_rec <- 
  recipe(Class ~ ., data = credit_train) %>%
  step_downsample(Class) %>%
  step_dummy(all_nominal(), -all_outcomes()) %>%
  step_zv(all_numeric()) %>%
  step_normalize(Time, log_amount)

# Create workflow so all models can replicate same analysis methods
credit_wf <- workflow() %>%
  add_recipe(credit_rec)
```

```{r}
# Specify logistic model
glm_spec <- logistic_reg() %>%
  set_engine("glm")

# Specify random forest model
rf_spec <- rand_forest(trees = 500) %>%
  set_engine("ranger") %>%
  set_mode("classification")
```

```{r}
doParallel::registerDoParallel()

# Fit logistic model to all folds in training data (resampling), saving certain metrics
glm_rs <- credit_wf %>%
  add_model(glm_spec) %>%
  fit_resamples(
    resamples = credit_folds,
    metrics = metric_set(roc_auc, accuracy, sensitivity, specificity, j_index),
    control = control_resamples(save_pred = TRUE)
  )

glm_rs
```

```{r}
doParallel::registerDoParallel()

# Fit random forest model to all folds in training data (resampling), saving certain metrics
rf_rs <- credit_wf %>%
  add_model(rf_spec) %>%
  fit_resamples(
    resamples = credit_folds,
    metrics = metric_set(roc_auc, accuracy, sensitivity, specificity, j_index),
    control = control_resamples(save_pred = TRUE)
  )

rf_rs
```


# Evaluate Models

```{r}
collect_metrics(glm_rs)
collect_metrics(rf_rs)
```

GLM has higher sensitivity, so will be better at detecting fraud.

```{r}
glm_rs %>%
  collect_predictions() %>%
  group_by(id) %>%
  roc_curve(Class, .pred_Fraud) %>%
  autoplot()
```

```{r}
# Fit final model to training data
credit_final <- credit_wf %>%
  add_model(glm_spec) %>%
  last_fit(credit_split)

collect_metrics(credit_final)

collect_predictions(credit_final) %>%
  conf_mat(Class, .pred_Fraud)
```

<!-- # Test Modelling -->
<!-- ## Oversampling -->

<!-- For oversampling, we'll use the ROSE (random over sampling examples) method  -->
<!-- [Menardi, G. and Torelli, N. (2014)](https://link.springer.com/article/10.1007/s10618-012-0295-5). -->

<!-- ## Logistic -->

<!-- ```{r} -->
<!-- set.seed(1) -->
<!-- cv_folds <- vfold_cv(credit, strata = "Class", repeats = 5) -->

<!-- cls_metrics <- metric_set(roc_auc, j_index) -->
<!-- ``` -->

<!-- ```{r} -->
<!-- log_mod <-  -->
<!--   logistic_reg() %>%  -->
<!--   set_engine("glm") -->
<!-- ``` -->

<!-- ```{r} -->
<!-- log_rose_wflw <-  -->
<!--   workflow() %>%  -->
<!--   add_model(log_mod) %>%  -->
<!--   add_recipe(credit_rec) -->

<!-- log_rose_wflw -->
<!-- ``` -->

<!-- ```{r} -->
<!-- log_wflw <-  -->
<!--   workflow() %>%  -->
<!--   add_model(log_mod) %>%  -->
<!--   add_formula(Class ~ .) -->
<!-- ``` -->

<!-- ```{r} -->
<!-- set.seed(2180) -->

<!-- log_rose_res <- fit_resamples( -->
<!--   log_rose_wflw,  -->
<!--   resamples = cv_folds,  -->
<!--   metrics = cls_metrics -->
<!-- ) -->

<!-- collect_metrics(log_rose_res) -->
<!-- ``` -->

<!-- ```{r} -->
<!-- set.seed(2180) -->

<!-- log_only_res <- fit_resamples( -->
<!--   log_wflw, -->
<!--   resamples = cv_folds, -->
<!--   metrics = cls_metrics -->
<!-- ) -->

<!-- collect_metrics(log_only_res) -->
<!-- ``` -->

<!-- ```{r} -->
<!-- no_sampling <-  -->
<!--   log_only_res %>%  -->
<!--   collect_metrics(summarize = FALSE) %>%  -->
<!--   dplyr::select(-.estimator) %>%  -->
<!--   mutate(sampling = "no_sampling") -->

<!-- with_sampling <-  -->
<!--   log_rose_res %>%  -->
<!--   collect_metrics(summarize = FALSE) %>%  -->
<!--   dplyr::select(-.estimator) %>%  -->
<!--   mutate(sampling = "rose") -->

<!-- bind_rows(no_sampling, with_sampling) %>%  -->
<!--   mutate(label = paste(id2, id)) %>%   -->
<!--   ggplot(aes(x = sampling, y = .estimate, group = label)) +  -->
<!--   geom_line(alpha = .4) +  -->
<!--   facet_wrap(~ .metric, scales = "free_y") -->
<!-- ``` -->

