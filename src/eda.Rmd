---
title: "STAT 508 Final Project EDA"
author: "Callum Arnold"
output:
    html_notebook:
        code_folding: hide
        toc: yes
        toc_float: yes
---

# Set Up

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, fig.width = 14)
```

```{r}
library(tidyverse)
library(here)
library(hrbrthemes)
library(janitor)
library(corrplot)
library(skimr)

RNGkind(sample.kind = "Rounding")
set.seed(1)

theme_set(theme_ipsum())
```

```{r}
credit <- as_tibble(read_csv(here("data", "creditcard.csv")))
```

# EDA

```{r}
head(credit)
```

## Variable Summary

```{r}
anyNA(credit)
```

```{r}
skim(credit)
```


```{r}
for (i in 1:ncol(credit)){
  print(summary(credit[, i]))
}
```

### Distributions

```{r}
for (i in names(credit[, -31])) {
  p <- ggplot(credit, aes_string(x = i)) +
    geom_density(fill = "cornsilk")

  print(p)
}
```

## Relationship with Response

```{r}
ggplot(data = credit, aes(x = Time, fill = factor(Class))) +
  geom_histogram() +
  facet_wrap(~Class, scales = "free")
```

```{r}
ggplot(data = credit, aes(x = log(Amount), fill = factor(Class))) +
  geom_histogram() +
  facet_wrap(~Class, scales = "free")
```

```{r}
ggplot(data = credit, aes(x = Time, y = log(Amount), alpha = 0.2, color = factor(Class))) +
  geom_point() +
  facet_wrap(~Class, scales = "free")
```
There is a clear skew to the `Amount` variable, so it is worth applying a transformation to the data. 
As there are values of `0`, we need to add `1` to ensure that we don't get `Inf` values produced after log transformation.

```{r}
credit <- credit %>%
  mutate(log_amount = log(Amount + 1))

summary(credit$log_amount)
```

```{r}
ggplot(credit, aes(x = log_amount)) +
    geom_density(fill = "cornsilk")
```

## Predictor Correlations

```{r}
corrplot(cor(credit[, -31]), method = "square", type = "upper")
```

Transformation of `Amount` has helped to reduce collinearities of the predictors.
The only correlations are between `V3` and `Time`, and `V2` and `log_amount`.
This is as expected as PCA produces orthogonal linear combinations,
therefore there shouldn't be much correlation between them.

```{r}
credit %>%
  mutate(
    trans_time = scale(Time, center = TRUE),
    trans_amount = scale(log_amount, center = TRUE)
    ) %>%
  select(trans_time, everything()) %>%
  select(-c(Amount, Time, log_amount)) %>%
  cor() %>%
  corrplot(method = "square", type = "upper")
```

# PCA Importance

Even though we don't have the original data for a full exploration of the PCs,
we can still look at the importance of the individual PCs in explaining the 
total variance in the data as we know that the variance observed in each PC
is equal to its eigenvalue (https://online.stat.psu.edu/stat505/lesson/11/11.2).
Therefore we can still calculate the relative proportions of variance explained,
without needed the eigenvalue outputs of the standard `prcomp` function.

```{r}
vars <- credit %>%
  select(-c(Time, Amount, log_amount, Class)) %>%
  sapply(var) %>%
  sort(decreasing = TRUE) %>%
  as_tibble()

vars
```

```{r}
prop_vars <- vars %>%
  transmute(variance = value) %>%
  mutate(prop_var = variance / sum(variance),
         cum_var = cumsum(prop_var),
         PC = fct_inorder(paste0("V", row_number())))

levels(prop_vars$PC)

prop_vars %>%
  ggplot(aes(x = PC, y = prop_var)) +
  geom_col() + 
  labs(title = "Proportion of Variance Explained by Principal Components",
       x = "Principal component number",
       y = "Proportion of variance explained")
```

```{r}
prop_vars %>%
  ggplot(aes(x = PC, y = cum_var)) +
  geom_col() + 
  labs(title = "Cumulative Proportion of Variance Explained by Principal Components",
       x = "Principal component number",
       y = "Proportion of variance explained")
```



